{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SOURCE = './source'\n",
    "SUSPICIOUS_DOCS = './test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./source/orig_taskd.txt',\n",
       " './source/orig_taske.txt',\n",
       " './source/orig_taska.txt',\n",
       " './source/orig_taskb.txt',\n",
       " './source/orig_taskc.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(os.path.join(DATA_SOURCE, '*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test/g0pC_taskc.txt',\n",
       " './test/g0pE_taskd.txt',\n",
       " './test/g0pE_taske.txt',\n",
       " './test/g0pC_taskb.txt',\n",
       " './test/g1pA_taske.txt',\n",
       " './test/g3pB_taskd.txt',\n",
       " './test/g3pB_taske.txt',\n",
       " './test/g0pC_taska.txt',\n",
       " './test/g1pA_taskd.txt',\n",
       " './test/g0pC_taske.txt',\n",
       " './test/g3pB_taska.txt',\n",
       " './test/g0pE_taskb.txt',\n",
       " './test/g0pE_taskc.txt',\n",
       " './test/g0pC_taskd.txt',\n",
       " './test/g1pA_taska.txt',\n",
       " './test/g1pA_taskc.txt',\n",
       " './test/g3pB_taskb.txt',\n",
       " './test/g0pE_taska.txt',\n",
       " './test/g3pB_taskc.txt',\n",
       " './test/g1pA_taskb.txt',\n",
       " './test/g2pA_taskb.txt',\n",
       " './test/g0pD_taskd.txt',\n",
       " './test/g0pB_taskc.txt',\n",
       " './test/g0pB_taskb.txt',\n",
       " './test/g0pD_taske.txt',\n",
       " './test/g2pA_taskc.txt',\n",
       " './test/g3pC_taskd.txt',\n",
       " './test/g2pA_taska.txt',\n",
       " './test/g0pB_taska.txt',\n",
       " './test/g3pC_taske.txt',\n",
       " './test/g3pC_taska.txt',\n",
       " './test/g2pA_taskd.txt',\n",
       " './test/g0pD_taskb.txt',\n",
       " './test/g0pB_taske.txt',\n",
       " './test/g0pB_taskd.txt',\n",
       " './test/g0pD_taskc.txt',\n",
       " './test/g2pA_taske.txt',\n",
       " './test/g3pC_taskb.txt',\n",
       " './test/g0pD_taska.txt',\n",
       " './test/g3pC_taskc.txt',\n",
       " './test/g2pB_taskc.txt',\n",
       " './test/g4pC_taskb.txt',\n",
       " './test/g0pA_taskb.txt',\n",
       " './test/g4pE_taske.txt',\n",
       " './test/g4pE_taskd.txt',\n",
       " './test/g0pA_taskc.txt',\n",
       " './test/g4pC_taskc.txt',\n",
       " './test/g2pB_taskb.txt',\n",
       " './test/g4pC_taska.txt',\n",
       " './test/g0pA_taska.txt',\n",
       " './test/g2pB_taska.txt',\n",
       " './test/g2pB_taske.txt',\n",
       " './test/g4pC_taskd.txt',\n",
       " './test/g4pE_taskc.txt',\n",
       " './test/g0pA_taskd.txt',\n",
       " './test/g0pA_taske.txt',\n",
       " './test/g4pE_taskb.txt',\n",
       " './test/g4pC_taske.txt',\n",
       " './test/g2pB_taskd.txt',\n",
       " './test/g4pE_taska.txt',\n",
       " './test/g4pD_taske.txt',\n",
       " './test/g2pE_taskd.txt',\n",
       " './test/g2pC_taskc.txt',\n",
       " './test/g4pB_taskb.txt',\n",
       " './test/g1pD_taska.txt',\n",
       " './test/g4pB_taskc.txt',\n",
       " './test/g2pC_taskb.txt',\n",
       " './test/g2pE_taske.txt',\n",
       " './test/g4pD_taskd.txt',\n",
       " './test/g1pB_taskd.txt',\n",
       " './test/g4pB_taska.txt',\n",
       " './test/g3pA_taske.txt',\n",
       " './test/g1pD_taskc.txt',\n",
       " './test/g1pD_taskb.txt',\n",
       " './test/g3pA_taskd.txt',\n",
       " './test/g2pC_taska.txt',\n",
       " './test/g1pB_taske.txt',\n",
       " './test/g4pD_taskc.txt',\n",
       " './test/g2pE_taskb.txt',\n",
       " './test/g1pB_taska.txt',\n",
       " './test/g2pC_taske.txt',\n",
       " './test/g4pB_taskd.txt',\n",
       " './test/g3pA_taska.txt',\n",
       " './test/g4pB_taske.txt',\n",
       " './test/g2pC_taskd.txt',\n",
       " './test/g2pE_taskc.txt',\n",
       " './test/g4pD_taskb.txt',\n",
       " './test/g2pE_taska.txt',\n",
       " './test/g1pB_taskb.txt',\n",
       " './test/g3pA_taskc.txt',\n",
       " './test/g1pD_taske.txt',\n",
       " './test/g1pD_taskd.txt',\n",
       " './test/g3pA_taskb.txt',\n",
       " './test/g1pB_taskc.txt',\n",
       " './test/g4pD_taska.txt']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob(os.path.join(SUSPICIOUS_DOCS, '*.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "def tokenize_file(file_name: str) -> list:\n",
    "    try:\n",
    "        text = codecs.open(file_name).read()\n",
    "    except UnicodeDecodeError:\n",
    "        text = codecs.open(file_name, encoding='cp1252').read()\n",
    "    return wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minhash(text, n_gram, is_file=True, num_perm=128):\n",
    "    \"\"\"\n",
    "    Generate minhash from single file or text given ngrams and number of permutation\n",
    "    \"\"\"\n",
    "    if is_file:\n",
    "        tokens = tokenize_file(text)\n",
    "    else:\n",
    "        tokens = wordpunct_tokenize(text)\n",
    "    minhash = MinHash(num_perm=num_perm)\n",
    "    for gram in nltk.ngrams(tokens, n_gram):\n",
    "        minhash.update(\" \".join(gram).encode('utf-8')) \n",
    "    return minhash\n",
    "\n",
    "def create_lsh(source_dir, threshold=.3, n_gram=3, num_perm=128, max_num=10) -> MinHashLSH:\n",
    "    \"\"\"\n",
    "    Create an LSH instance from text files in source directory given threshold, n-grams, and number of permutation\n",
    "    Returns an LSH object and a list of source file numbers \n",
    "    \"\"\"\n",
    "    lsh = MinHashLSH(num_perm=num_perm, threshold=threshold)#, \n",
    "                   #storage_config={'type': 'redis', 'redis': {'host': 'localhost', 'port': 6379, 'db': 1},'name': 1})\n",
    "    file_names = glob(os.path.join(source_dir, '*.txt'))[:max_num]\n",
    "    keys = []\n",
    "    minhashes = []\n",
    "    for fname in file_names:\n",
    "        minhash = create_minhash(fname, n_gram, num_perm=num_perm)\n",
    "        keys.append(re.findall(r'task\\w', fname)[0])\n",
    "        minhashes.append(minhash)\n",
    "    with lsh.insertion_session() as session:\n",
    "        for key, minhash in zip(keys, minhashes):\n",
    "            session.insert(key, minhash)\n",
    "    return lsh, keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_single(lsh, text_dir, n_gram, num_perm=128):\n",
    "    minhash = create_minhash(text_dir, n_gram, is_file=True, num_perm=num_perm)\n",
    "    result = lsh.query(minhash)\n",
    "    return result\n",
    "\n",
    "def eval_list(sus_dir, lsh, n_gram, num_perm):\n",
    "    results = {}\n",
    "    for text_dir in glob(os.path.join(sus_dir, '*.txt')):\n",
    "        results[re.findall(\"g[A-Za-z0-9_-]*\", text_dir)[0]] = eval_single(lsh, text_dir, n_gram, num_perm=num_perm)\n",
    "    return results\n",
    "        \n",
    "def find_accuracy(keys, results):\n",
    "    correct = 0\n",
    "    for doc in results:\n",
    "        if len(results[doc])>0 and results[doc][0] in doc:\n",
    "            correct += 1\n",
    "    return correct/len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6526315789473685"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram = 3\n",
    "jaccard_th = .05\n",
    "num_perm = 128\n",
    "lsh, keys = create_lsh(DATA_SOURCE, n_gram=n_gram, threshold=jaccard_th, max_num=10, num_perm=num_perm)\n",
    "results = eval_list(SUSPICIOUS_DOCS, lsh, n_gram, num_perm)\n",
    "find_accuracy(keys, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 grams, 0.0 < jaccard, accuracy: 0.2736842105263158\n",
      "2 grams, 0.01 < jaccard, accuracy: 0.2736842105263158\n",
      "2 grams, 0.02 < jaccard, accuracy: 0.28421052631578947\n",
      "2 grams, 0.05 < jaccard, accuracy: 0.4842105263157895\n",
      "2 grams, 0.1 < jaccard, accuracy: 0.5684210526315789\n",
      "3 grams, 0.0 < jaccard, accuracy: 0.5789473684210527\n",
      "3 grams, 0.01 < jaccard, accuracy: 0.5789473684210527\n",
      "3 grams, 0.02 < jaccard, accuracy: 0.6631578947368421\n",
      "3 grams, 0.05 < jaccard, accuracy: 0.6526315789473685\n",
      "3 grams, 0.1 < jaccard, accuracy: 0.4105263157894737\n",
      "4 grams, 0.0 < jaccard, accuracy: 0.5789473684210527\n",
      "4 grams, 0.01 < jaccard, accuracy: 0.5789473684210527\n",
      "4 grams, 0.02 < jaccard, accuracy: 0.5473684210526316\n",
      "4 grams, 0.05 < jaccard, accuracy: 0.5263157894736842\n",
      "4 grams, 0.1 < jaccard, accuracy: 0.3684210526315789\n",
      "5 grams, 0.0 < jaccard, accuracy: 0.5578947368421052\n",
      "5 grams, 0.01 < jaccard, accuracy: 0.5578947368421052\n",
      "5 grams, 0.02 < jaccard, accuracy: 0.5578947368421052\n",
      "5 grams, 0.05 < jaccard, accuracy: 0.5052631578947369\n",
      "5 grams, 0.1 < jaccard, accuracy: 0.35789473684210527\n",
      "6 grams, 0.0 < jaccard, accuracy: 0.5368421052631579\n",
      "6 grams, 0.01 < jaccard, accuracy: 0.5368421052631579\n",
      "6 grams, 0.02 < jaccard, accuracy: 0.5263157894736842\n",
      "6 grams, 0.05 < jaccard, accuracy: 0.4631578947368421\n",
      "6 grams, 0.1 < jaccard, accuracy: 0.3368421052631579\n",
      "7 grams, 0.0 < jaccard, accuracy: 0.5157894736842106\n",
      "7 grams, 0.01 < jaccard, accuracy: 0.5157894736842106\n",
      "7 grams, 0.02 < jaccard, accuracy: 0.5157894736842106\n",
      "7 grams, 0.05 < jaccard, accuracy: 0.4105263157894737\n",
      "7 grams, 0.1 < jaccard, accuracy: 0.28421052631578947\n",
      "8 grams, 0.0 < jaccard, accuracy: 0.47368421052631576\n",
      "8 grams, 0.01 < jaccard, accuracy: 0.47368421052631576\n",
      "8 grams, 0.02 < jaccard, accuracy: 0.43157894736842106\n",
      "8 grams, 0.05 < jaccard, accuracy: 0.37894736842105264\n",
      "8 grams, 0.1 < jaccard, accuracy: 0.25263157894736843\n",
      "9 grams, 0.0 < jaccard, accuracy: 0.47368421052631576\n",
      "9 grams, 0.01 < jaccard, accuracy: 0.47368421052631576\n",
      "9 grams, 0.02 < jaccard, accuracy: 0.4421052631578947\n",
      "9 grams, 0.05 < jaccard, accuracy: 0.4105263157894737\n",
      "9 grams, 0.1 < jaccard, accuracy: 0.2736842105263158\n"
     ]
    }
   ],
   "source": [
    "for n_gram in range(2, 10):\n",
    "    for jaccard_th in [.0, .01, .02, .05, .1]:\n",
    "        lsh, keys = create_lsh(DATA_SOURCE, n_gram=n_gram, threshold=jaccard_th, max_num=10, num_perm=num_perm)\n",
    "        results = eval_list(SUSPICIOUS_DOCS, lsh, n_gram, num_perm)\n",
    "        acc = find_accuracy(keys, results)\n",
    "        print(f'{n_gram} grams, {jaccard_th} < jaccard, accuracy: {acc}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c94922506fb676f83812e148df9e303d1c678c5ed3aaf1a9305c9d04cd8ba7f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
